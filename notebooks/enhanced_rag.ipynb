{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fc17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spyder-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Claude suggested I add in the system path to help find the py module. See Appendix K. \n",
    "import sys\n",
    "sys.path.append('/Users/shoyou100/Visual-Code-Studio-workspace/assignment2-rag/src')\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import pandas as pd\n",
    "from naive_rag import NaiveRAG  \n",
    "from evaluation import Evaluation\n",
    "from enhanced_rag import EnhancedRAG\n",
    "from datasets import Dataset\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ab245",
   "metadata": {},
   "source": [
    "# Read Passages from the Datasets and Drop rows if they are NA or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff0e11a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uruguay (official full name in  ; pron.  , Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is bordered by Brazil to the north, by Arge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montevideo was founded by the Spanish in the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The economy is largely based in agriculture (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Transparency International, Urugu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              passage\n",
       "id                                                   \n",
       "0   Uruguay (official full name in  ; pron.  , Eas...\n",
       "1   It is bordered by Brazil to the north, by Arge...\n",
       "2   Montevideo was founded by the Spanish in the e...\n",
       "3   The economy is largely based in agriculture (m...\n",
       "4   According to Transparency International, Urugu..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\")\n",
    "\n",
    "print(passages.shape)\n",
    "passages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222f1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(918, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question     answer\n",
       "id                                                              \n",
       "0   Was Abraham Lincoln the sixteenth President of...        yes\n",
       "2   Did Lincoln sign the National Banking Act of 1...        yes\n",
       "4                    Did his mother die of pneumonia?         no\n",
       "6       How many long was Lincoln's formal education?  18 months\n",
       "8        When did Lincoln begin his political career?       1832"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\")\n",
    "print(queries.shape)\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed65698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude recommended I incorporate chunking using LangChain. See Appendix M.\n",
    "advanced_rag = EnhancedRAG(200,25)\n",
    "\n",
    "chunk_metadata = []  # Track which passage each chunk came from\n",
    "\n",
    "for idx, passage_text in enumerate(passages['passage']):\n",
    "    chunks = advanced_rag.splitTexts(passage_text)\n",
    "    advanced_rag.addChunks(chunks)\n",
    "    \n",
    "    # Keep track of source passage for each chunk\n",
    "    for chunk in chunks:\n",
    "        chunk_metadata.append({\n",
    "            'passage_id': passages['id'][idx] if 'id' in passages else idx,\n",
    "            'chunk_text': chunk\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61b2b881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original passages: 3200\n",
      "Total chunks: 8476\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original passages: {len(passages)}\")\n",
    "print(f\"Total chunks: {advanced_rag.getChunkLength()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "536601b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n",
      "Tokenizer loaded.\n",
      "Schema created.\n",
      "Seq2Seq model loaded.\n",
      "Milvus client connected.\n",
      "Dropping existing collection 'rag_mini'...\n",
      "Collection dropped.\n",
      "384\n",
      "[[-0.00375673  0.03796374 -0.03182048 ... -0.04027529  0.00659006\n",
      "   0.03112675]\n",
      " [-0.03231501 -0.0094521  -0.10944731 ... -0.01333902 -0.0162666\n",
      "  -0.01005705]\n",
      " [ 0.00882224  0.0258874  -0.01894972 ... -0.03792767  0.07239034\n",
      "   0.00896565]\n",
      " ...\n",
      " [-0.00906186 -0.04410971 -0.11035763 ... -0.03789582  0.03106767\n",
      "   0.06000853]\n",
      " [ 0.02661895  0.05995833 -0.08694381 ...  0.0077695   0.01058571\n",
      "  -0.01678987]\n",
      " [-0.00681225  0.05897506 -0.05983922 ... -0.01872636 -0.01558293\n",
      "   0.01479083]]\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "naive_rag = NaiveRAG('all-MiniLM-L6-v2', 'google/flan-t5-small', 'rag_wikipedia_mini.db', 'rag_mini')\n",
    "\n",
    "passage_embeddings = naive_rag.embedding_model.encode(advanced_rag.getChunks()) \n",
    "print(naive_rag.embedding_model.get_sentence_embedding_dimension())\n",
    "query_embeddings = naive_rag.embedding_model.encode(queries['question'].tolist())\n",
    "print(query_embeddings)\n",
    "print(naive_rag.embedding_model.get_sentence_embedding_dimension())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354e711",
   "metadata": {},
   "source": [
    "# Create Milvus Client and Insert your Embeddings to your DB\n",
    "- Make sure you define a schema for your collection (Points will be deducted if you fail to define a proper schema with ids, passage text, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7fc9faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage_id</th>\n",
       "      <th>chunk_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Uruguay (official full name in  ; pron.  , Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.7 million live in the capital Montevideo and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>It is bordered by Brazil to the north, by Arge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Atlantic Ocean to the southeast. It is the sec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Montevideo was founded by the Spanish in the e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passage_id                                         chunk_text\n",
       "0           0  Uruguay (official full name in  ; pron.  , Eas...\n",
       "1           0  1.7 million live in the capital Montevideo and...\n",
       "2           1  It is bordered by Brazil to the north, by Arge...\n",
       "3           1  Atlantic Ocean to the southeast. It is the sec...\n",
       "4           2  Montevideo was founded by the Spanish in the e..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Claude recommended I convert the chunks to a dataframe and use that for database creation. See Appendix N.\n",
    "chunks_df = pd.DataFrame(chunk_metadata)\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3c3a286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns defined.\n",
      "Fields added to schema.\n",
      "Collection created.\n",
      "Data inserted successfully.\n",
      "Index created successfully.\n",
      "Collection loaded into memory\n"
     ]
    }
   ],
   "source": [
    "# Redefine id_ to ensure it is available\n",
    "id_ = chunks_df.index.tolist()\n",
    "passage = chunks_df['chunk_text'].tolist()\n",
    "embedding = passage_embeddings.tolist()\n",
    "naive_rag.create_dataBase(chunks_df, 'chunk_text', passage_embeddings, 384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc5d8681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity count: 8476\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'functions': [], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': True}\n"
     ]
    }
   ],
   "source": [
    "naive_rag.sanityCheck('rag_mini')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87920ab",
   "metadata": {},
   "source": [
    "# Steps to Fetch Results\n",
    "- Read the Question Dataset\n",
    "- Clean the Question Dataset if necessary (Drop Questions with NaN etc.)\n",
    "- Convert Each Query to a Vector Embedding (Use the same embedding model you used to embed your document)\n",
    "- Try for a Single Question First\n",
    "- Load Collection into Memory after creating Index for Search on your embedding field (This is an essential step before you can search in your db)\n",
    "- Search and Fetch Top N Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b68ab",
   "metadata": {},
   "source": [
    "**Develop your Prompt**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023b43c",
   "metadata": {},
   "source": [
    "# Generate Responses for 100 queries in the Dataset.\n",
    "# Top-1 Passage Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc262ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['no'], ['18 months'], ['1832'], ['United:'], ['Grace Bedell'], ['1789'], ['yes'], ['yes'], ['yes'], ['yes'], ['Hardin'], ['1861'], ['Abraham Lincoln'], ['yes'], ['John C. Pemberton'], ['Because Lincoln was a rebel.'], ['yes'], ['yes'], ['yes'], ['18'], ['1846'], ['23-year'], [\"The Farmer's Almanac\"], ['yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro'], ['a priest'], ['1821'], ['twenty years'], ['yes'], ['yes'], ['yes'], ['yes'], ['King Victor Emmanuel III was there to pay homage to Avogadro.'], ['It allows chemists to determine the exact amounts of substances:'], ['The theory was never published'], ['no'], ['Mississippi, and severed the rail line to Vicksburg'], ['Carolus Linnaeus'], ['Vercelli'], ['1693'], ['yes'], ['no'], ['yes'], ['yes'], ['yes'], ['Anders Celsius'], ['named after him'], ['The Celsius crater on the Moon is named after him.'], ['No'], ['Celsius was born in Uppsala in Sweden.'], ['no'], ['He died in 1774'], ['the persecution of Port-Royal had ceased'], ['yes'], ['Yes'], ['yes'], ['head, the thorax, and the abdomen'], ['mimicry'], ['Leptinotarsa decemlineata'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['insects'], ['beetles are a common insect pest.'], ['yes'], ['yes'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['no'], ['Some species'], ['The elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['Yes.'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['Yes. Coleopterists have formed organisations to facilitate the study of beetles. '], ['A single female lay from several dozen to several thousand eggs during her lifetime.'], ['Yes'], ['develop resistance to'], ['yes'], ['yes'], ['yes'], ['May 18, 2007'], ['1905'], ['Roaring Twenties'], ['graduated'], ['Northampton'], ['the Supreme Court'], ['yes'], ['no'], ['no'], ['August 27, 1881'], ['Plymouth']]\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 1 - Instruction Prompt\n",
    "system_prompt = f\"You are a concise and accurate assistant. Answer the question based on the provided context.\"\n",
    "naive_rag.queries_list = []  # Reset queries_list before new searches\n",
    "naive_rag.contexts_list = [] \n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row,3, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1b77e",
   "metadata": {},
   "source": [
    "# Finding out the Basic QA Metrics (F1 score, EM score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "767cc301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Score: 25/100 = 0.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_rag.calculateEM(queries.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52ba1f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.31292990342990346\n"
     ]
    }
   ],
   "source": [
    "# F1 Score Calculation\n",
    "evaluator = Evaluation()\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9088c25b",
   "metadata": {},
   "source": [
    "# Persona Styles 2 & 3 - Top-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7272a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['no'], ['18 months'], ['1832'], ['United:'], ['Grace Bedell'], ['1789'], ['yes'], ['yes'], ['yes'], ['yes'], ['Hardin'], ['1861'], ['Abraham Lincoln'], ['Lincoln was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['John C. Pemberton'], ['to protect rebels'], ['yes'], ['yes'], ['Lincoln was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['1816'], ['1846'], ['23-year'], ['the sands'], ['yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro'], ['john d. scott'], ['1821'], ['twenty years'], ['yes'], ['yes'], ['yes'], ['yes'], ['King Victor Emmanuel III was there to pay homage to Avogadro.'], ['It allows chemists to determine the exact amounts of substances:'], ['The theory was never published'], ['no'], ['Mississippi, and severed the rail line to Vicksburg'], ['Carolus Linnaeus'], ['Vercelli'], ['1693'], ['no'], ['no'], ['yes'], ['yes'], ['yes'], ['Anders Celsius'], ['named after him'], ['The Celsius crater on the Moon is named after him.'], ['yes'], ['Celsius was born in Uppsala in Sweden.'], ['no'], ['He died in Scotland'], ['the persecution of Port-Royal had ceased'], ['yes'], ['no'], ['yes'], ['head, the thorax, and the abdomen'], ['mimicry'], ['Leptinotarsa decemlineata'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['insects'], ['beetles are beetles'], ['yes'], ['uniform'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['no'], ['Some species'], ['elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['There is a thriving industry in the collection of beetle specimens for amateur and'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['yes'], ['a female may lay from several dozen to several thousand eggs during her lifetime'], ['yes'], ['develop resistance to'], ['yes'], ['yes'], ['yes'], ['May 18, 2007'], ['1905'], ['Roaring Twenties'], ['graduate'], ['Northampton'], ['john d. w. s. w. s. w.'], ['yes'], ['no'], ['yes'], ['August 27, 1881'], ['Plymouth']]\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 2 - Persona Prompt\n",
    "system_prompt = f\"You are a concise history teacher.\"\n",
    "\n",
    "naive_rag.queries_list = []  # Reset the queries list\n",
    "naive_rag.contexts_list = [] \n",
    "\n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row, 3, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d1b9304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Score: 24/100 = 0.2400\n",
      "F1 Score: 0.3050965700965701\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 2 Evaluation\n",
    "naive_rag.calculateEM(queries.answer)\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e21da63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['no'], ['18 months'], ['1832'], ['provided government grants for agricultural universities in each state'], ['Grace Bedell'], ['1789'], ['yes'], ['no'], ['yes'], ['Yes'], ['Hardin'], ['In 1891 Lincoln was appointed President of the United States.'], ['Abraham Lincoln'], ['He was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['John C. Pemberton'], ['To protect rebels'], ['yes'], ['yes'], ['He was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['1816'], ['1846'], ['23-year'], [\"Lincoln used a Farmers' Almanac in the northwestern part of the country\"], ['Yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro, Count'], ['ordained'], ['1821'], ['for another twenty years'], ['yes'], ['He is most noted for his contributions to the theory of molarity and molecular:'], ['Yes'], ['yes'], ['King Victor Emmanuel III was there to pay homage to Avogadro.'], ['It allows chemists to determine the exact amounts of substances:'], ['The theory was never published'], ['yes'], ['Mississippi, and severed the rail line to Vicksburg. which was completed in 1800.'], ['Carolus Linnaeus'], ['Vercelli'], ['1693'], ['yes'], ['no'], ['He was professor of astronomy at Uppsala University from 1730 to 1744'], ['Yes'], ['yes'], ['Anders Celsius'], ['named after him'], ['The Celsius crater on the Moon is named after him.'], ['yes'], ['He was professor of astronomy at Uppsala University from 1730 to 1744'], ['no'], ['Fact1: The year of toil was 1775-1785; Fact2: The year of'], ['The persecution of Port-Royal had ceased'], ['yes'], ['They interact with their ecosystems in several ways. They often feed on plants and fungi,'], ['yes'], ['the head, the thorax, and the abdomen'], ['mimicry'], ['The Colorado potato beetle, Leptinotarsa decemlineat'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['An agricultural, forestry, and household insect pests'], ['a grasshopper'], ['Yes'], ['yes'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['yes'], ['Some species'], ['The elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['Yes'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['Yes'], ['A single female lay from several dozen to several thousand eggs during her lifetime .'], ['Yes'], ['many of which it has begun to develop resistance to'], ['yes'], ['John Calvin Coolidge Jr. was born in Plymouth, Windsor County, Vermont, on July'], ['Yes'], ['upon graduating from college'], ['1905'], ['Roaring Twenties'], ['He was an Amherst undergraduate'], ['Northampton'], ['Chief Justice'], ['yes'], ['No'], ['no'], ['The relevant sentence in the passage is:'], ['Plymouth']]\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 3 - CoT\n",
    "system_prompt = f\"Read the question first. Then retrieve the context from the database. Finally, answer the question using the retrieved context.\"\n",
    "naive_rag.queries_list = []  # Reset the queries list\n",
    "naive_rag.contexts_list = [] \n",
    "\n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row, 3, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab7e45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Score: 26/100 = 0.2600\n",
      "F1 Score: 0.3081832141500334\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 3 Evaluation\n",
    "naive_rag.calculateEM(queries.answer)\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e81b32",
   "metadata": {},
   "source": [
    "# Top-10 Passage Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bff4c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['yes'], ['18 months'], ['1832'], ['United'], ['Grace Bedell'], ['1851'], ['yes'], ['yes'], ['yes'], ['yes'], ['Hardin'], ['1861'], ['Abraham Lincoln'], ['yes'], ['John C. Pemberton'], ['Because Lincoln was a rebel.'], ['yes'], ['yes'], ['yes'], ['18'], ['1846'], ['23-year'], [\"Lincoln's Almanac\"], ['yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro'], ['Abraham Lincoln'], ['1821'], ['twenty years'], ['yes'], ['yes'], ['no'], ['yes'], ['No'], [\"Avogadro 's number is commonly used to compute the results of chemical reactions\"], ['The theory was never published'], ['Yes'], ['Mississippi, and severed the rail line to Vicksburg'], ['Carolus Linnaeus'], ['Roosevelt'], ['1693'], ['yes'], ['no'], ['yes'], ['no'], ['yes'], ['Celsius'], ['Volta'], ['The observatory of Anders Celsius, from a contemporary engraving.'], ['yes'], ['No'], ['no'], ['Middle East'], ['the persecution of Port-Royal had ceased'], ['yes'], ['Yes'], ['yes'], ['head, the thorax, and the abdomen'], ['active defence'], ['Leptinotarsa decemlineata'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['insects'], ['beetles are a common insect pest.'], ['yes'], ['yes'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['no'], ['Some species'], ['The elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['Yes.'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['Yes.'], ['A single female lay from several dozen to several thousand eggs during her lifetime.'], ['Yes'], ['damage crops'], ['yes'], ['yes'], ['yes'], ['May 18, 2007'], ['1892'], ['Roosevelt explained, \"There is an intimate relation between our streams and the development and conservation of all the'], ['graduated'], ['Northampton'], ['Adams'], ['yes'], ['no'], ['no'], ['August 27, 1881'], ['Plymouth']]\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 1 - Instruction Prompt\n",
    "system_prompt = f\"You are a concise and accurate assistant. Answer the question based on the provided context.\"\n",
    "naive_rag.queries_list = []  # Reset queries_list before new searches\n",
    "naive_rag.contexts_list = [] \n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row,10, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e9f179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EM Score: 20/100 = 0.2000\n",
      "F1 Score: 0.29570093042604484\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 1 Evaluation\n",
    "naive_rag.calculateEM(queries.answer)\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3495eeef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['yes'], ['18 months'], ['1832'], ['United'], ['Grace Bedell'], ['1851'], ['yes'], ['yes'], ['yes'], ['yes'], ['Hardin'], ['1861'], ['Abraham Lincoln'], ['Lincoln was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['John C. Pemberton'], ['to protect rebels'], ['yes'], ['yes'], ['Lincoln was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['1816'], ['1846'], ['23-year'], [\"Lincoln's Almanac\"], ['yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro'], ['Abraham Lincoln'], ['1821'], ['twenty years'], ['yes'], ['no'], ['no'], ['yes'], ['Avogadro submitted his poem to a French journal.'], [\"Avogadro 's number is commonly used to compute the results of chemical reactions\"], ['The theory was never published'], ['yes'], ['Mississippi, and severed the rail line to Vicksburg'], ['Carolus Linnaeus'], ['Roosevelt'], ['1693'], ['no'], ['no'], ['yes'], ['no'], ['yes'], ['Celsius'], ['Volta'], ['The observatory of Anders Celsius, from a contemporary engraving.'], ['Celsius was the first to perform and publish careful experiments aiming at the definition of an international temperature scale'], ['Celsius was born in Uppsala in Sweden.'], ['no'], ['Middle East'], ['the persecution of Port-Royal had ceased'], ['yes'], ['no'], ['yes'], ['head, the thorax, and the abdomen'], ['active defence'], ['Leptinotarsa decemlineata'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['insects'], ['beetles are beetles'], ['yes'], ['uniform'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['no'], ['Some species'], ['elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['There is a thriving industry in the collection of beetle specimens for amateur and'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['They interact with their ecosystems in several ways. They often feed on plants and fungi,'], ['a female may lay from several dozen to several thousand eggs during her lifetime'], ['yes'], ['damage crops'], ['yes'], ['yes'], ['yes'], ['May 18, 2007'], ['1892'], ['development and conservation of all the other great permanent sources of wealth'], ['graduate'], ['Northampton'], ['Adams'], ['yes'], ['no'], ['yes'], ['August 27, 1881'], ['Plymouth']]\n",
      "EM Score: 18/100 = 0.1800\n",
      "F1 Score: 0.2834877506678422\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 2 - Persona Prompt\n",
    "system_prompt = f\"You are a concise history teacher.\"\n",
    "\n",
    "naive_rag.queries_list = []  # Reset the queries list\n",
    "naive_rag.contexts_list = [] \n",
    "\n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row, 10, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)\n",
    "\n",
    "# Prompt Style 2 Evaluation\n",
    "naive_rag.calculateEM(queries.answer)\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f50cd83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['yes'], ['yes'], ['yes'], ['18 months'], ['1832'], ['provided government grants for agricultural universities in each state'], ['Grace Bedell'], ['1851'], ['yes'], ['no'], ['yes'], ['Yes'], ['Hardin'], ['In 1891 Lincoln was appointed President of the United States.'], ['Abraham Lincoln'], ['He was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['John C. Pemberton'], ['To protect rebels'], ['yes'], ['yes'], ['He was eventually chosen as the Republican candidate for the 1860 election for several reasons.'], ['1816'], ['1846'], ['23-year'], [\"Lincoln used a Farmers' Almanac in the northwestern part of the country\"], ['Yes'], ['no'], ['Lorenzo Romano Amedeo Carlo Avogadro, Count'], ['Abraham Lincoln'], ['1821'], ['for another twenty years'], ['yes'], ['no'], ['no'], ['yes'], ['Avogadro submitted his poem to a French journal.'], [\"Avogadro 's number is commonly used to compute the results of chemical reactions\"], ['The theory was never published'], ['Yes'], ['Mississippi, and severed the rail line to Vicksburg. which was completed in 1800.'], ['He'], ['Roosevelt'], ['1693'], ['yes'], ['no'], ['It is instead believed that both the Finnish and the Swedish language arrived in Finland at:'], ['no'], ['yes'], ['He was professor of astronomy at Uppsala University from 1730 to 1744'], ['Volta'], ['The observatory of Anders Celsius, from a contemporary engraving.'], ['yes'], ['It is instead believed that both the Finnish and the Swedish language arrived in Finland at:'], ['no'], ['the Middle East'], ['The persecution of Port-Royal had ceased'], ['yes'], ['They interact with their ecosystems in several ways. They often feed on plants and fungi,'], ['yes'], ['the head, the thorax, and the abdomen'], ['active defence'], ['The Colorado potato beetle, Leptinotarsa decemlineat'], ['by their hardened, often darkened head, the presence of chewing mouthparts, and'], ['An agricultural, forestry, and household insect pests'], ['a grasshopper'], ['Yes'], ['yes'], ['yes'], ['350,000'], ['if there has been enough rain to produce a large quantity of green vegetation'], ['coleopterology'], ['yes'], ['Polyphaga is the largest suborder, containing more than 300,000 described species in more'], ['yes'], ['Some species'], ['The elytra'], ['Antennae'], ['yes'], ['yes'], ['tracheal system'], ['Yes'], ['Coleopterists have formed organisations to facilitate the study of beetles.'], ['Yes'], ['A single female lay from several dozen to several thousand eggs during her lifetime .'], ['Yes'], ['damaged crops'], ['yes'], ['John Calvin Coolidge Jr. was born in Plymouth, Windsor County, Vermont, on July'], ['Yes'], ['upon graduating from college'], ['1892'], ['the development and conservation of all the other great permanent sources of wealth'], ['He was an Amherst undergraduate'], ['Northampton'], ['Adams'], ['yes'], ['No'], ['no'], ['The relevant sentence in the passage is:'], ['Plymouth']]\n",
      "EM Score: 19/100 = 0.1900\n",
      "F1 Score: 0.27526777651749046\n"
     ]
    }
   ],
   "source": [
    "# Prompt Style 3 - CoT\n",
    "system_prompt = f\"Read the question first. Then retrieve the context from the database. Finally, answer the question using the retrieved context.\"\n",
    "naive_rag.queries_list = []  # Reset the queries list\n",
    "naive_rag.contexts_list = [] \n",
    "\n",
    "# Top-1 Retrieval with Generation for all queries\n",
    "count = 0\n",
    "for row in queries.question:\n",
    "   naive_rag.search(row, 10, passage, system_prompt, use_enhanced=True, enhancedRag=advanced_rag)\n",
    "   count += 1\n",
    "   if count == 100:\n",
    "      break\n",
    "\n",
    "print(naive_rag.queries_list)\n",
    "\n",
    "# Prompt Style 3 Evaluation\n",
    "naive_rag.calculateEM(queries.answer)\n",
    "f1_score = evaluator.compute_f1(naive_rag.flatten_answer, queries.answer.tolist())\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef21cbf",
   "metadata": {},
   "source": [
    "# Advanced Evaluation using RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e75b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude code helped provide the structure for the data and for the evaluation code below. See Appendix S.\n",
    "data = {\n",
    "    \"question\": queries.question[:100].tolist() ,                     # Question\n",
    "    \"answer\": naive_rag.flatten_answer ,                       # Generated Answer\n",
    "    \"contexts\": naive_rag.contexts_list ,                     # Context you pass in. You can just use top-1 here\n",
    "    \"reference\": [truth for truth in queries.answer[:100].tolist()]                  # Reference Answer in the dataset (Human annotated)\n",
    "}\n",
    "\n",
    "# Convert dict to dataset\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32f0a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1759543245.259053  820528 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "E0000 00:00:1759543245.273760  820528 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "/var/folders/f0/3bfdym850v72lx6m176k5jk80000gn/T/ipykernel_65288/3640797945.py:14: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  gemini_embeddings = LangchainEmbeddingsWrapper(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
      "E0000 00:00:1759543245.276608  820528 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]E0000 00:00:1759543245.631088  820528 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
      "Exception raised in Job[5]: IndexError(list index out of range)\n",
      "Evaluating:   0%|          | 1/400 [00:03<22:38,  3.40s/it]Exception raised in Job[1]: IndexError(list index out of range)\n",
      "Exception raised in Job[13]: IndexError(list index out of range)\n",
      "Exception raised in Job[9]: IndexError(list index out of range)\n",
      "Evaluating:   3%|▎         | 13/400 [00:08<03:26,  1.87it/s]Exception raised in Job[25]: IndexError(list index out of range)\n",
      "Exception raised in Job[21]: IndexError(list index out of range)\n",
      "Evaluating:   6%|▋         | 26/400 [00:12<02:06,  2.95it/s]Exception raised in Job[29]: IndexError(list index out of range)\n",
      "Exception raised in Job[33]: IndexError(list index out of range)\n",
      "Exception raised in Job[37]: IndexError(list index out of range)\n",
      "Evaluating:  12%|█▏        | 49/400 [00:26<03:10,  1.85it/s]Exception raised in Job[57]: IndexError(list index out of range)\n",
      "Exception raised in Job[61]: IndexError(list index out of range)\n",
      "Evaluating:  15%|█▌        | 61/400 [00:30<02:18,  2.44it/s]Exception raised in Job[65]: IndexError(list index out of range)\n",
      "Exception raised in Job[69]: IndexError(list index out of range)\n",
      "Evaluating:  16%|█▌        | 63/400 [00:33<02:57,  1.90it/s]Exception raised in Job[73]: IndexError(list index out of range)\n",
      "Evaluating:  18%|█▊        | 70/400 [00:35<02:19,  2.36it/s]Exception raised in Job[77]: IndexError(list index out of range)\n",
      "Evaluating:  19%|█▉        | 75/400 [00:37<02:12,  2.45it/s]Exception raised in Job[81]: IndexError(list index out of range)\n",
      "Evaluating:  21%|██▏       | 85/400 [00:43<02:33,  2.05it/s]Exception raised in Job[93]: IndexError(list index out of range)\n",
      "Exception raised in Job[97]: IndexError(list index out of range)\n",
      "Evaluating:  24%|██▍       | 98/400 [00:46<01:41,  2.98it/s]Exception raised in Job[105]: IndexError(list index out of range)\n",
      "Exception raised in Job[101]: IndexError(list index out of range)\n",
      "Exception raised in Job[109]: IndexError(list index out of range)\n",
      "Evaluating:  27%|██▋       | 108/400 [00:52<02:13,  2.19it/s]Exception raised in Job[117]: IndexError(list index out of range)\n",
      "Evaluating:  30%|██▉       | 119/400 [00:56<01:35,  2.94it/s]Exception raised in Job[129]: IndexError(list index out of range)\n",
      "Exception raised in Job[133]: IndexError(list index out of range)\n",
      "Evaluating:  33%|███▎      | 132/400 [01:03<01:44,  2.57it/s]Exception raised in Job[141]: IndexError(list index out of range)\n",
      "Exception raised in Job[137]: IndexError(list index out of range)\n",
      "Exception raised in Job[145]: IndexError(list index out of range)\n",
      "Evaluating:  36%|███▌      | 142/400 [01:08<01:49,  2.36it/s]Exception raised in Job[149]: IndexError(list index out of range)\n",
      "Evaluating:  36%|███▋      | 146/400 [01:10<01:49,  2.31it/s]Exception raised in Job[153]: IndexError(list index out of range)\n",
      "Evaluating:  38%|███▊      | 154/400 [01:12<01:21,  3.03it/s]Exception raised in Job[161]: IndexError(list index out of range)\n",
      "Exception raised in Job[157]: IndexError(list index out of range)\n",
      "Exception raised in Job[165]: IndexError(list index out of range)\n",
      "Evaluating:  40%|████      | 161/400 [01:16<01:43,  2.31it/s]Exception raised in Job[169]: IndexError(list index out of range)\n",
      "Evaluating:  42%|████▏     | 168/400 [01:18<01:21,  2.85it/s]Exception raised in Job[177]: IndexError(list index out of range)\n",
      "Exception raised in Job[173]: IndexError(list index out of range)\n",
      "Evaluating:  45%|████▌     | 181/400 [01:25<01:49,  2.01it/s]Exception raised in Job[193]: IndexError(list index out of range)\n",
      "Exception raised in Job[189]: IndexError(list index out of range)\n",
      "Evaluating:  48%|████▊     | 194/400 [01:29<01:08,  2.99it/s]Exception raised in Job[205]: IndexError(list index out of range)\n",
      "Exception raised in Job[201]: IndexError(list index out of range)\n",
      "Evaluating:  52%|█████▏    | 208/400 [01:38<01:18,  2.45it/s]Exception raised in Job[213]: IndexError(list index out of range)\n",
      "Exception raised in Job[217]: IndexError(list index out of range)\n",
      "Exception raised in Job[221]: IndexError(list index out of range)\n",
      "Evaluating:  54%|█████▎    | 214/400 [01:42<01:39,  1.87it/s]Exception raised in Job[225]: IndexError(list index out of range)\n",
      "Evaluating:  56%|█████▌    | 222/400 [01:45<01:15,  2.34it/s]Exception raised in Job[233]: IndexError(list index out of range)\n",
      "Exception raised in Job[229]: IndexError(list index out of range)\n",
      "Evaluating:  58%|█████▊    | 233/400 [01:48<00:59,  2.82it/s]Exception raised in Job[237]: IndexError(list index out of range)\n",
      "Evaluating:  59%|█████▉    | 236/400 [01:48<00:49,  3.32it/s]Exception raised in Job[241]: IndexError(list index out of range)\n",
      "Exception raised in Job[245]: IndexError(list index out of range)\n",
      "Evaluating:  59%|█████▉    | 237/400 [01:51<01:30,  1.80it/s]Exception raised in Job[249]: IndexError(list index out of range)\n",
      "Evaluating:  62%|██████▏   | 246/400 [01:53<00:51,  2.97it/s]Exception raised in Job[253]: IndexError(list index out of range)\n",
      "Exception raised in Job[257]: IndexError(list index out of range)\n",
      "Evaluating:  64%|██████▍   | 255/400 [01:55<00:43,  3.31it/s]Exception raised in Job[261]: IndexError(list index out of range)\n",
      "Evaluating:  64%|██████▍   | 256/400 [01:57<01:01,  2.34it/s]Exception raised in Job[265]: IndexError(list index out of range)\n",
      "Evaluating:  66%|██████▌   | 262/400 [01:59<00:52,  2.63it/s]Exception raised in Job[269]: IndexError(list index out of range)\n",
      "Evaluating:  67%|██████▋   | 267/400 [02:00<00:44,  3.01it/s]Exception raised in Job[273]: IndexError(list index out of range)\n",
      "Evaluating:  70%|███████   | 282/400 [02:08<00:45,  2.60it/s]Exception raised in Job[293]: IndexError(list index out of range)\n",
      "Evaluating:  71%|███████   | 283/400 [02:09<00:53,  2.18it/s]Exception raised in Job[289]: IndexError(list index out of range)\n",
      "Evaluating:  74%|███████▍  | 298/400 [02:17<00:43,  2.34it/s]Exception raised in Job[309]: IndexError(list index out of range)\n",
      "Exception raised in Job[305]: IndexError(list index out of range)\n",
      "Exception raised in Job[301]: IndexError(list index out of range)\n",
      "Evaluating:  80%|████████  | 321/400 [02:30<00:36,  2.17it/s]Exception raised in Job[329]: IndexError(list index out of range)\n",
      "Exception raised in Job[325]: IndexError(list index out of range)\n",
      "Exception raised in Job[333]: IndexError(list index out of range)\n",
      "Evaluating:  84%|████████▎ | 334/400 [02:36<00:22,  2.91it/s]Exception raised in Job[341]: IndexError(list index out of range)\n",
      "Exception raised in Job[345]: IndexError(list index out of range)\n",
      "Exception raised in Job[337]: IndexError(list index out of range)\n",
      "Evaluating:  86%|████████▌ | 342/400 [02:40<00:23,  2.45it/s]Exception raised in Job[353]: IndexError(list index out of range)\n",
      "Evaluating:  89%|████████▉ | 356/400 [02:50<00:27,  1.58it/s]Exception raised in Job[365]: IndexError(list index out of range)\n",
      "Exception raised in Job[369]: IndexError(list index out of range)\n",
      "Evaluating:  90%|█████████ | 362/400 [02:52<00:21,  1.74it/s]Exception raised in Job[381]: IndexError(list index out of range)\n",
      "Evaluating:  92%|█████████▏| 369/400 [02:54<00:12,  2.39it/s]Exception raised in Job[377]: IndexError(list index out of range)\n",
      "Exception raised in Job[373]: IndexError(list index out of range)\n",
      "Evaluating:  96%|█████████▌| 382/400 [02:58<00:06,  2.99it/s]Exception raised in Job[389]: IndexError(list index out of range)\n",
      "Exception raised in Job[385]: IndexError(list index out of range)\n",
      "Exception raised in Job[393]: IndexError(list index out of range)\n",
      "Evaluating: 100%|██████████| 400/400 [03:07<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAs Evaluation Results:\n",
      "{'faithfulness': 0.5483, 'answer_relevancy': 0.6813, 'context_recall': 0.5600, 'context_precision': 0.5100}\n"
     ]
    }
   ],
   "source": [
    "# Pass the dataset above to the evaluate method in RAGAs\n",
    "# Your code here\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemini_llm = LangchainLLMWrapper(ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0))\n",
    "gemini_embeddings = LangchainEmbeddingsWrapper(GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate using Gemini\n",
    "results = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ],\n",
    "    llm=gemini_llm,\n",
    "    embeddings=gemini_embeddings,\n",
    ")\n",
    "\n",
    "print(\"RAGAs Evaluation Results:\")\n",
    "print(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spyder-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
